
# Introduction Ã  Linux pour l'Intelligence Artificielle

Apprenez Ã  utiliser Linux pour **dÃ©velopper**, **entraÃ®ner** et **dÃ©ployer** des projets dâ€™IA.  
Ce guide est pensÃ© pour Ãªtre **pratique**, mÃªme si vous nâ€™Ãªtes pas expert en Linux.

---

## Ressources complÃ©mentaires

Les exercices, scripts dâ€™installation et configurations dÃ©taillÃ©es sont disponibles dans le repository associÃ© Ã  ce cours.  
ğŸ‘‰ Pas besoin de tout taper Ã  la main : vous pouvez rÃ©utiliser directement les fichiers fournis.

---

## Table des matiÃ¨res

1. Pourquoi utiliser Linux pour lâ€™IA ?
2. Les outils dâ€™IA disponibles sous Linux
3. Quelles distributions choisir pour dÃ©marrer ?
4. GÃ©rer ses environnements Python
5. Utiliser le GPU pour aller plus vite
6. Isoler ses projets avec Docker ou Kubernetes
7. Les bibliothÃ¨ques indispensables
8. Organiser et stocker ses donnÃ©es
9. DÃ©ployer un modÃ¨le en production
10. Surveiller et optimiser ses entraÃ®nements
11. Exemples pratiques et scripts prÃªts Ã  lâ€™emploi

---

## 1. Pourquoi utiliser Linux pour lâ€™IA ?

Linux est **le systÃ¨me prÃ©fÃ©rÃ© des chercheurs et ingÃ©nieurs IA**.  
Pourquoi ? Parce quâ€™il est :  

- **Rapide et optimisÃ©** : il gÃ¨re bien le CPU et le GPU.  
- **Ouvert et gratuit** : tout est open source, donc pas de licences coÃ»teuses.  
- **Flexible** : il fonctionne aussi bien sur un PC portable que sur un cluster gÃ©ant.  
- **Reproductible** : la mÃªme configuration peut Ãªtre utilisÃ©e partout (local, cloud, HPC).  

ğŸ’¡ Exemple : si vous entraÃ®nez un modÃ¨le PyTorch sur votre machine, vous pouvez facilement rÃ©utiliser le mÃªme environnement sur un serveur cloud.

---

## 2. Les outils dâ€™IA disponibles sous Linux

Voici les grands outils que vous croiserez :  

- **Applications** : Jupyter (notebooks), MLflow (suivi dâ€™expÃ©riences), Streamlit et Gradio (interfaces simples pour vos modÃ¨les).  
- **Frameworks IA** : PyTorch, TensorFlow, scikit-learn, XGBoost.  
- **Calcul parallÃ¨le** : CUDA (NVIDIA), ROCm (AMD), Dask ou Ray pour distribuer vos calculs.  
- **Infrastructure** : Docker (containers), Kubernetes (orchestration), SLURM (HPC).  

---

## 3. Quelles distributions choisir pour dÃ©marrer ?

- **Ubuntu 22.04 LTS** : la plus simple pour commencer (bonne doc, compatible GPU).  
- **Pop!\_OS** : pratique sur poste de travail, surtout si vous avez une carte graphique NVIDIA.  
- **Rocky / AlmaLinux** : pour les serveurs (stables, compatibles RHEL).  
- **Lambda Stack** : pensÃ©e pour lâ€™IA, avec PyTorch/TensorFlow dÃ©jÃ  installÃ©s.  

ğŸ‘‰ Conseil : commencez avec **Ubuntu**, câ€™est la plus utilisÃ©e.

---

## 4. GÃ©rer ses environnements Python

Le problÃ¨me classique : *â€œÃ§a marchait hier, mais plus aujourdâ€™huiâ€¦â€* â†’ conflits de versions.  
Pour Ã©viter Ã§a :  

- **Conda/Mamba** : parfait pour tester et explorer (un environnement par projet).  
- **Poetry** : mieux si vous dÃ©veloppez une librairie partagÃ©e.  
- **Docker + venv** : idÃ©al pour la production (images stables et portables).  

---

## 5. Utiliser le GPU pour aller plus vite

En IA, le GPU est **indispensable** pour entraÃ®ner rapidement vos modÃ¨les.  

- **NVIDIA (CUDA)** : le plus courant et le plus mature (ex. cuDNN, TensorRT).  
- **AMD (ROCm)** : une alternative ouverte, mais dÃ©pend du matÃ©riel.  
- **Multi-GPU** : si vous avez plusieurs cartes, vous pouvez les faire travailler ensemble avec NCCL, Gloo ou MPI.  

ğŸ’¡ Testez si PyTorch voit votre GPU :  
```python
import torch
print(torch.cuda.is_available())
````

---

## 6. Isoler ses projets avec Docker ou Kubernetes

* **Docker** : crÃ©e des environnements reproductibles â†’ *â€œÃ§a marche chez moiâ€ devient â€œÃ§a marche partoutâ€*.
* **Kubernetes** : utile quand on a beaucoup de projets Ã  dÃ©ployer ou des jobs dâ€™entraÃ®nement Ã  distribuer.
* **Singularity** : recommandÃ© dans les clusters HPC oÃ¹ on nâ€™a pas les droits administrateurs.

---

## 7. Les bibliothÃ¨ques indispensables

* **Machine Learning** : scikit-learn, XGBoost, LightGBM.
* **Deep Learning** : PyTorch (souvent prÃ©fÃ©rÃ©), TensorFlow/Keras.
* **Visualisation** : Matplotlib, Seaborn, Plotly.
* **Suivi dâ€™expÃ©riences** : MLflow, Weights & Biases.

---

## 8. Organiser et stocker ses donnÃ©es

Bonnes pratiques :

* SÃ©parer **brut / traitÃ© / rÃ©sultats**.
* Versionner avec **DVC** ou **Git LFS**.
* Utiliser des formats efficaces comme **Parquet**.
* Stocker les artefacts (modÃ¨les, checkpoints) dans **S3, GCS ou MinIO**.

---

## 9. DÃ©ployer un modÃ¨le en production

* **Serving** : FastAPI, TorchServe, TensorFlow Serving, Triton.
* **Automatisation (CI/CD)** : GitHub Actions ou GitLab CI pour tester, builder et dÃ©ployer automatiquement.
* **Surveillance** : Prometheus + Grafana pour les mÃ©triques et alertes.

---

## 10. Surveiller et optimiser ses entraÃ®nements

* **Sur la machine** : `htop` (CPU/RAM), `nvidia-smi` (GPU).
* **Pour les modÃ¨les** : profiler PyTorch/TensorFlow, quantification, pruning, ONNX.
* **Optimisation GPU** : TensorRT pour accÃ©lÃ©rer lâ€™infÃ©rence.

---

## 11. Exemples pratiques et scripts prÃªts Ã  lâ€™emploi

Le repo contient des scripts dÃ©jÃ  prÃªts pour tester :

* `scripts/env_cpu.sh` â†’ crÃ©er un environnement Python minimal (CPU).
* `scripts/pytorch_cuda.sh` â†’ installer PyTorch + CUDA et tester le GPU.
* `scripts/mlflow_local.sh` â†’ lancer MLflow en local.
* `docker/Dockerfile` â†’ exemple dâ€™image Docker pour servir un modÃ¨le.

ğŸ‘‰ Vous pouvez directement exÃ©cuter ces fichiers pour gagner du temps.

---

```



