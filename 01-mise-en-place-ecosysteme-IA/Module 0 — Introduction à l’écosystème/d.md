
<h1 id="intro-linux-ia">Introduction √† Linux pour l'Intelligence Artificielle</h1>

Apprenez √† utiliser Linux pour **d√©velopper**, **entra√Æner** et **d√©ployer** des projets d‚ÄôIA.  
Pas besoin d‚Äô√™tre un expert syst√®me : ce guide va droit au but avec des explications simples et des exemples concrets.

<h2 id="ressources">Ressources compl√©mentaires</h2>

Les exercices, scripts d‚Äôinstallation et configurations d√©taill√©es sont fournis dans le repository associ√© √† ce cours.  
üí° Vous pouvez directement ex√©cuter les scripts sans tout retaper √† la main.

<h2 id="toc">Table des mati√®res</h2>

1. [Pourquoi utiliser Linux pour l‚ÄôIA ?](#pourquoi-linux-ia)  
2. [Les outils d‚ÄôIA disponibles sous Linux](#ecosysteme-ia)  
3. [Quelles distributions choisir pour d√©marrer ?](#distributions-ia)  
4. [Comment g√©rer ses environnements Python](#environnements-python)  
5. [Acc√©l√©rer ses calculs avec un GPU](#acceleration-gpu)  
6. [Isoler ses projets avec des containers](#containers-ia)  
7. [Les biblioth√®ques essentielles](#frameworks-bibliotheques)  
8. [Organiser et stocker ses donn√©es](#gestion-donnees)  
9. [D√©ployer un mod√®le en production](#deploiement-production)  
10. [Surveiller et optimiser ses entra√Ænements](#monitoring-optimisation)  
11. [Exemples pratiques](#annexe-exemples-pratiques)  

<h2 id="pourquoi-linux-ia">1. Pourquoi utiliser Linux pour l‚ÄôIA ?</h2>

Linux est le syst√®me le plus utilis√© par les chercheurs et ing√©nieurs IA.  

Pourquoi ?  
- **Rapide** : il g√®re tr√®s bien CPU et GPU.  
- **Ouvert** : tout est open source et gratuit.  
- **Flexible** : fonctionne du simple PC portable jusqu‚Äôau supercalculateur.  
- **Fiable** : on peut reproduire la m√™me configuration partout (local, cloud, cluster).  

**Cycle de vie typique d‚Äôun projet IA** :

```mermaid
flowchart LR
  A[Exploration] --> B[Pr√©paration des donn√©es]
  B --> C[Entra√Ænement]
  C --> D[√âvaluation]
  D --> E[Packaging]
  E --> F[D√©ploiement]
  F --> G[Surveillance & Am√©liorations]
  G --> B
````

<h2 id="ecosysteme-ia">2. Les outils d‚ÄôIA disponibles sous Linux</h2>

* **Applications** : Jupyter (notebooks interactifs), MLflow (suivi d‚Äôexp√©riences), Streamlit/Gradio (interfaces web rapides).
* **Frameworks** : PyTorch, TensorFlow, scikit-learn, XGBoost.
* **Calcul parall√®le** : CUDA (NVIDIA), ROCm (AMD), Dask ou Ray (calcul distribu√©).
* **Infrastructure** : Docker (containers), Kubernetes (orchestration), SLURM (HPC).

**Vue d‚Äôensemble** :

```mermaid
mindmap
root((Linux & IA))
  Applications
    Jupyter
    MLflow
    Streamlit
    Gradio
  Frameworks
    PyTorch
    TensorFlow
    Scikit-learn
    XGBoost
  Parall√©lisation
    CUDA
    ROCm
    Dask
    Ray
  Infra
    Docker
    Kubernetes
    SLURM
    Singularity
```

<h2 id="distributions-ia">3. Quelles distributions choisir pour d√©marrer ?</h2>

üëâ Si vous d√©butez, prenez **Ubuntu 22.04 LTS** : simple, bien document√©, compatible GPU.

Autres choix possibles :

| Distribution      | Cas d‚Äôusage    | Points forts              |
| ----------------- | -------------- | ------------------------- |
| Rocky/AlmaLinux 9 | Serveurs/HPC   | Stabilit√©, proche de RHEL |
| Pop!_OS           | PC personnel   | Excellent support GPU     |
| Lambda Stack      | IA cl√© en main | PyTorch/TensorFlow + CUDA |
| Debian            | Usage g√©n√©ral  | Solide et tr√®s stable     |

<h2 id="environnements-python">4. Comment g√©rer ses environnements Python</h2>

Le probl√®me classique : *‚Äú√ßa marchait hier mais plus aujourd‚Äôhui‚Ä¶‚Äù*
C‚Äôest souvent √† cause de conflits de versions.

* **Conda/Mamba** : pratique pour tester, chaque projet a son environnement.
* **Poetry** : utile pour des biblioth√®ques partag√©es.
* **Docker + venv** : en production, pour avoir un environnement identique partout.

<h2 id="acceleration-gpu">5. Acc√©l√©rer ses calculs avec un GPU</h2>

Un GPU est essentiel pour l‚ÄôIA moderne.

* **NVIDIA (CUDA)** : le plus utilis√© (cuDNN, TensorRT, PyTorch-CUDA).
* **AMD (ROCm)** : alternative ouverte, mais d√©pend du mat√©riel.
* **Multi-GPU** : on peut entra√Æner sur plusieurs cartes avec NCCL, Gloo, MPI.

üí° V√©rifier si PyTorch voit le GPU :

```python
import torch
print(torch.cuda.is_available())
```

<h2 id="containers-ia">6. Isoler ses projets avec des containers</h2>

* **Docker** : √©viter les ‚Äú√ßa marche chez moi mais pas chez toi‚Äù.
* **Kubernetes** : pour g√©rer plusieurs containers et d√©ployer √† grande √©chelle.
* **Singularity** : utilis√© en HPC, pas besoin d‚Äô√™tre administrateur.

<h2 id="frameworks-bibliotheques">7. Les biblioth√®ques essentielles</h2>

* **Machine Learning** : scikit-learn, XGBoost, LightGBM.
* **Deep Learning** : PyTorch (souvent pr√©f√©r√©), TensorFlow/Keras.
* **Visualisation** : Matplotlib, Seaborn, Plotly.
* **Suivi d‚Äôexp√©riences** : MLflow, Weights & Biases.

<h2 id="gestion-donnees">8. Organiser et stocker ses donn√©es</h2>

Bonnes pratiques :

* S√©parer **donn√©es brutes**, **donn√©es trait√©es**, et **r√©sultats**.
* Versionner avec **DVC** ou **Git LFS**.
* Utiliser des formats efficaces comme **Parquet**.
* Stocker mod√®les et checkpoints sur **S3/GCS/MinIO**.

<h2 id="deploiement-production">9. D√©ployer un mod√®le en production</h2>

* **Serving** : FastAPI, TorchServe, TensorFlow Serving, Triton.
* **Automatisation (CI/CD)** : GitHub Actions ou GitLab CI pour tester et d√©ployer.
* **Monitoring** : Prometheus + Grafana pour surveiller m√©triques et alertes.

**Cha√Æne typique** :

```mermaid
flowchart TB
  Code[Code + Mod√®le] --> Build[Construction de l'image Docker]
  Build --> Registry[Registry]
  Registry --> Deploy[Kubernetes]
  Deploy --> Ingress[Acc√®s clients]
  Ingress --> Utilisateurs
  Deploy --> Obs[Logs & Metrics]
```

<h2 id="monitoring-optimisation">10. Surveiller et optimiser ses entra√Ænements</h2>

* **Sur la machine** : `htop` (CPU), `nvidia-smi` (GPU).
* **Pour les mod√®les** : profiler PyTorch/TensorFlow, quantification, pruning, ONNX.
* **Optimisation** : TensorRT pour acc√©l√©rer l‚Äôinf√©rence.

<h2 id="annexe-exemples-pratiques">11. Exemples pratiques</h2>

Dans le dossier du repo :

* `scripts/env_cpu.sh` ‚Üí cr√©er un environnement Python minimal (CPU).
* `scripts/pytorch_cuda.sh` ‚Üí installer PyTorch + CUDA et tester le GPU.
* `scripts/mlflow_local.sh` ‚Üí lancer MLflow en local.
* `docker/Dockerfile` ‚Üí exemple d‚Äôimage Docker pour servir un mod√®le.

```


Veux-tu que je te propose aussi un **fichier `SUMMARY.md`** (style GitBook / MkDocs) pour g√©n√©rer automatiquement un menu lat√©ral avec ces ancres ?
```
