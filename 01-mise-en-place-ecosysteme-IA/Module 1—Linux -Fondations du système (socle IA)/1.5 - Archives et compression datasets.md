# Archives et compression datasets

Gérer efficacement les gros datasets et archives en IA

## Note importante

Les corrections des exercices pratiques de ce module sont disponibles dans le repository GitHub du cours mentionné dans le document d'introduction (0.1).

## Table des matières

1. [Formats d'archives](#formats-archives)
2. [Création et extraction](#creation-extraction)
3. [Gestion de gros datasets](#gros-datasets)
4. [Optimisation espace disque](#optimisation-espace)

## Formats d'archives

### Théorie : Choisir le bon format

En IA, vous manipulerez des datasets volumineux, modèles pré-entraînés, et sauvegardes. Chaque format d'archive a ses avantages selon le contexte : vitesse, compression, compatibilité.

### Formats courants

#### tar.gz - Standard Unix

| Aspect | Description | Usage IA |
|--------|-------------|----------|
| **Compression** | Bonne (gzip) | Datasets moyens |
| **Vitesse** | Rapide | Archives fréquentes |
| **Compatibilité** | Universelle | Partage entre systèmes |
| **Streaming** | Possible | Transferts réseau |

#### tar.bz2 - Compression élevée

| Aspect | Description | Usage IA |
|--------|-------------|----------|
| **Compression** | Excellente | Archivage long terme |
| **Vitesse** | Plus lente | Stockage pas urgent |
| **CPU** | Intensif | Serveurs puissants |
| **Économie espace** | Maximale | Stockage coûteux |

#### zip - Compatibilité Windows

| Aspect | Description | Usage IA |
|--------|-------------|----------|
| **Compatibilité** | Multi-plateforme | Collaboration mixte |
| **Compression** | Moyenne | Compromis universel |
| **Extraction partielle** | Possible | Gros datasets |
| **Métadonnées** | Préservées | Datasets organisés |

### Formats spécialisés

#### 7z - Compression maximale

**Avantages :**
- Ratio compression excellent
- Support multiples algorithmes
- Chiffrement intégré
- Gestion mémoire optimisée

**Usage IA :**
- Archives long terme
- Datasets très volumineux
- Backup sécurisés
- Optimisation stockage cloud

#### tar.xz - Moderne et efficace

**Caractéristiques :**
- Compression LZMA2 avancée
- Ratio excellent avec vitesse correcte
- Support parallélisme
- Standard moderne Linux

### Critères de choix

#### Selon le contexte IA

| Contexte | Format recommandé | Raison |
|----------|------------------|---------|
| **Dataset quotidien** | tar.gz | Équilibre vitesse/taille |
| **Archive long terme** | tar.bz2 ou tar.xz | Compression maximale |
| **Transfert réseau** | tar.gz | Vitesse prioritaire |
| **Stockage limité** | 7z | Compression optimale |
| **Collaboration** | zip | Compatibilité universelle |

### Exemples pratiques

**Scénario : Archiver différents types de données IA**

Types de données et formats :
- **Images** (JPEG/PNG) : tar.gz (déjà compressées)
- **Texte brut** (CSV, TXT) : tar.bz2 (compression élevée)
- **Modèles** (pickles, checkpoints) : tar.xz (équilibre)
- **Logs** (fichiers texte) : tar.bz2 (compression maximale)

### Pratique non corrigée

**Exercice 1 : Test formats**
- Créez des archives de mêmes données en différents formats
- Comparez tailles finales et temps de création
- Testez extraction partielle selon format
- Documentez vos recommandations

**Exercice 2 : Cas d'usage réels**
- Simulez archivage de datasets variés
- Choisissez format optimal selon contexte
- Testez performance sur votre système
- Créez vos standards personnels

## Création et extraction

### Théorie : Maîtriser les commandes d'archivage

La ligne de commande offre un contrôle fin sur l'archivage : compression sélective, préservation permissions, gestion erreurs, progress monitoring.

### Commandes tar

#### Création d'archives

| Commande | Description | Usage IA |
|----------|-------------|----------|
| **tar -czf archive.tar.gz dir/** | Créer gzip | Datasets standards |
| **tar -cjf archive.tar.bz2 dir/** | Créer bzip2 | Compression max |
| **tar -cJf archive.tar.xz dir/** | Créer xz | Moderne efficace |
| **tar -cf - dir/ \| gzip > file.gz** | Pipeline custom | Contrôle fin |

#### Options avancées

| Option | Description | Bénéfice IA |
|--------|-------------|-------------|
| **-v** | Mode verbeux | Suivi progression |
| **--exclude pattern** | Exclure fichiers | Filtrer cache/tmp |
| **-p** | Préserver permissions | Sécurité datasets |
| **--sparse** | Fichiers creux | Économie espace |

### Extraction d'archives

#### Commandes d'extraction

| Commande | Description | Usage |
|----------|-------------|-------|
| **tar -xzf archive.tar.gz** | Extraire gzip | Standard |
| **tar -xjf archive.tar.bz2** | Extraire bzip2 | Archives compressées |
| **tar -tf archive.tar.gz** | Lister contenu | Vérification avant extraction |
| **tar -xzf archive.tar.gz file** | Extraction sélective | Gros datasets |

#### Gestion sécurisée

**Bonnes pratiques :**
- Vérifier contenu avant extraction complète
- Extraire dans répertoire dédié
- Vérifier espace disque disponible
- Valider intégrité après extraction

### Compression avancée

#### Optimisation par type

**Stratégies spécialisées :**
- **Images** : pas de recompression
- **Texte** : compression maximale
- **Binaires** : compression standard
- **Datasets mixtes** : compression adaptative

#### Parallélisation

**Outils accélérés :**
- **pigz** : gzip parallèle
- **pbzip2** : bzip2 parallèle
- **pixz** : xz parallèle
- **tar --use-compress-program** : intégration

### Exemples pratiques

**Scénario : Sauvegarde projet IA complet**

Structure d'archivage :
1. **Code source** : tar.gz (fichiers texte nombreux)
2. **Modèles entraînés** : tar.xz (gros fichiers binaires)
3. **Datasets** : selon type et usage
4. **Logs/résultats** : tar.bz2 (compression maximale)

### Monitoring et validation

#### Suivi progression

**Techniques utiles :**
- **pv** : barre de progression
- **--checkpoint** : points de contrôle tar
- **tee** : logging simultané
- **time** : mesure performance

#### Validation intégrité

**Vérifications essentielles :**
- Checksums avant/après
- Test extraction partielle
- Vérification taille attendue
- Validation structure directories

### Pratique non corrigée

**Exercice 3 : Archivage avancé**
- Créez archives avec options spécialisées
- Pratiquez exclusion de patterns
- Testez extraction sélective
- Mesurez performance selon options

**Exercice 4 : Pipeline d'archivage**
- Créez script d'archivage intelligent
- Implémentez choix automatique format
- Ajoutez validation et monitoring
- Testez sur datasets volumineux

## Gestion de gros datasets

### Théorie : Défis des données volumineuses

En IA, les datasets peuvent atteindre des téraoctets. Gérer ces volumes nécessite des stratégies spécifiques : archivage incrémental, streaming, distribution, optimisation réseau.

### Stratégies pour gros volumes

#### Archivage incrémental

**Approches possibles :**
- **tar --listed-incremental** : Sauvegardes différentielles
- **rsync + compression** : Synchronisation puis archivage
- **Split archives** : Division en parties gérables
- **Checksums par chunk** : Validation par parties

#### Streaming et pipelines

**Méthodes efficaces :**
- **tar | compression | transfert** : Pipeline complet
- **ssh + tar** : Archivage distant direct
- **socat** : Streaming réseau avancé
- **zstd** : Compression streaming rapide

### Outils spécialisés

#### GNU Parallel

**Usage pour datasets :**
- Compression parallèle par répertoire
- Traitement simultané multiples archives
- Distribution charge sur CPU cores
- Optimisation I/O simultanées

#### Split et chunking

| Commande | Usage | Bénéfice |
|----------|-------|----------|
| **split -b 1G file** | Division par taille | Gestion mémoire |
| **split -l 1000000 file** | Division par lignes | Traitement par batch |
| **tar ... \| split -b 2G** | Archives par chunks | Upload cloud |

### Optimisation réseau

#### Transferts efficaces

**Techniques avancées :**
- **rsync --compress** : Compression à la volée
- **scp -C** : SSH avec compression
- **tar \| ssh** : Pipeline direct distant
- **BitTorrent sync** : Distribution P2P

#### Reprise de transfert

**Mécanismes robustes :**
- **rsync --partial** : Reprise interruptions
- **wget -c** : Continue downloads
- **curl --continue-at** : Reprise uploads
- **Checksums** : Validation intégrité

### Gestion mémoire

#### Techniques économes

**Approches low-memory :**
- **Streaming processing** : Pas de chargement complet
- **mmap** : Mapping mémoire virtuelle
- **Lazy loading** : Chargement à la demande
- **Chunked processing** : Traitement par blocs

#### Monitoring ressources

**Surveillance nécessaire :**
- **Memory usage** : Éviter OOM killer
- **Disk I/O** : Prévenir saturation
- **Network bandwidth** : Optimiser transferts
- **CPU load** : Équilibrer compression

### Exemples pratiques

**Scénario : Traiter dataset ImageNet complet**

Stratégie complète :
1. **Analyse** : Taille totale et structure
2. **Chunking** : Division par classes/batches
3. **Compression** : Parallèle par chunk
4. **Validation** : Checksums et intégrité
5. **Distribution** : Réplication sécurisée

### Automatisation workflows

#### Scripts intelligents

**Fonctionnalités avancées :**
- Détection automatique taille optimale chunks
- Choix adaptatif compression selon type
- Parallélisation selon ressources disponibles
- Recovery automatique en cas d'erreur

#### Monitoring avancé

**Métriques importantes :**
- Progression globale et par chunk
- Vitesse instantanée et moyenne
- Prédiction temps restant
- Détection anomalies performance

### Pratique non corrigée

**Exercice 5 : Gros datasets**
- Simulez dataset multi-gigabytes
- Implémentez stratégie chunking
- Testez compression parallèle
- Optimisez selon vos ressources

**Exercice 6 : Pipeline complet**
- Créez workflow de bout en bout
- Intégrez validation et recovery
- Automatisez monitoring
- Documentez performance obtenue

## Optimisation espace disque

### Théorie : Gérer l'espace efficacement

L'IA consomme énormément d'espace : datasets, modèles, checkpoints, logs. Une gestion optimale de l'espace est cruciale pour maintenir productivité et coûts sous contrôle.

### Audit d'utilisation

#### Outils d'analyse

| Commande | Information | Usage IA |
|----------|-------------|----------|
| **du -sh \*** | Taille par répertoire | Identifier gros datasets |
| **df -h** | Espace libre filesystems | Planifier archives |
| **ncdu** | Analyse interactive | Exploration détaillée |
| **find . -size +1G** | Gros fichiers | Candidats compression |

#### Métriques importantes

**Indicateurs clés :**
- **Taille totale projets** : Planification stockage
- **Croissance temporelle** : Prédiction besoins
- **Ratio données/code** : Optimisation structure
- **Fichiers temporaires** : Nettoyage automatique

### Techniques de compression

#### Compression différentielle

**Méthodes avancées :**
- **delta compression** : Différences entre versions
- **deduplication** : Élimination doublons
- **sparse files** : Fichiers avec trous
- **hardlinks** : Partage fichiers identiques

#### Optimisation par type

| Type de données | Technique | Gain typique |
|-----------------|-----------|--------------|
| **Logs texte** | bzip2/xz | 80-90% |
| **Images JPEG** | Archivage seul | 5-10% |
| **Modèles PyTorch** | zstd/xz | 60-70% |
| **CSV datasets** | Parquet + compression | 70-80% |

### Stratégies de nettoyage

#### Nettoyage automatique

**Cibles prioritaires :**
- **Cache pip/conda** : Packages téléchargés
- **Checkpoints intermédiaires** : Sauvegardes temporaires
- **Logs anciens** : Historiques d'entraînement
- **Datasets temporaires** : Données de test

#### Politiques de rétention

**Règles recommandées :**
- **Données actives** : Accès rapide, pas de compression
- **Données récentes** : Compression légère, accès moyen
- **Archives anciennes** : Compression maximale
- **Backup long terme** : Stockage externe/cloud

### Stockage hiérarchique

#### Niveaux de stockage

| Niveau | Caractéristiques | Usage IA |
|--------|------------------|----------|
| **Hot storage** | SSD rapide | Données actives |
| **Warm storage** | HDD standard | Datasets occasionnels |
| **Cold storage** | Archivage | Backup long terme |
| **Frozen storage** | Cloud/tape | Conformité légale |

#### Migration automatique

**Critères de migration :**
- **Âge dernière utilisation** : Auto-archivage
- **Fréquence d'accès** : Optimisation placement
- **Taille vs coût** : Calcul ROI stockage
- **Importance projet** : Priorités business

### Outils d'optimisation

#### Compression avancée

**Outils spécialisés :**
- **zstd** : Compression rapide et efficace
- **lz4** : Ultra-rapide pour données chaudes
- **brotli** : Optimisé pour texte/web
- **zopfli** : Compression maximale gzip

#### Deduplication

**Solutions disponibles :**
- **fdupes** : Détection doublons
- **rdfind** : Hardlinks automatiques
- **duperemove** : Déduplication filesystem
- **borg** : Backup avec déduplication

### Exemples pratiques

**Scénario : Optimiser environnement de recherche IA**

Plan d'optimisation :
1. **Audit initial** : Cartographie utilisation
2. **Classification** : Hot/warm/cold data
3. **Compression** : Selon patterns d'accès
4. **Automatisation** : Politiques de nettoyage
5. **Monitoring** : Suivi évolution dans le temps

### Monitoring continu

#### Alertes automatiques

**Seuils recommandés :**
- **85% espace utilisé** : Warning
- **95% espace utilisé** : Critical
- **Croissance > 10%/jour** : Investigation
- **Fichiers > âge limite** : Auto-archivage

#### Reporting régulier

**Métriques de suivi :**
- Évolution utilisation espace
- Efficacité compression par type
- Coûts stockage par projet
- Performance accès données

### Pratique non corrigée

**Exercice 7 : Audit complet**
- Analysez utilisation espace de votre système
- Identifiez opportunités d'optimisation
- Implémentez compression ciblée
- Mesurez gains obtenus

**Exercice 8 : Automatisation nettoyage**
- Créez scripts de nettoyage automatique
- Implémentez politiques de rétention
- Configurez monitoring espace
- Testez récupération d'urgence

**Exercice 9 : Optimisation avancée**
- Explorez techniques de déduplication
- Testez compression différentielle
- Implémentez stockage hiérarchique
- Documentez stratégie globale

## Conclusion

### Points clés à retenir

- **Formats** : Choisir selon contexte (vitesse vs compression)
- **Commandes** : Maîtriser tar avec options avancées
- **Gros volumes** : Stratégies chunking et parallélisation
- **Optimisation** : Monitoring et nettoyage automatique

### Compétences acquises

Après ce module, vous devriez être capable de :
- Choisir le format d'archive optimal selon le contexte
- Créer et gérer des archives de datasets volumineux
- Optimiser l'utilisation de l'espace disque
- Automatiser la gestion du stockage pour l'IA

### Prochaines étapes

Le module suivant couvrira la gestion des utilisateurs et la sécurité de base, essentiels pour sécuriser vos environnements IA en production.
